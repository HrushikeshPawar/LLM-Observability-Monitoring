{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.otel import register\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.indices.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\"OPENAI_API_KEY\" in os.environ), \"Please set your OPENAI_API_KEY environment variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('..', 'data')\n",
    "SENTENCE_INDEX_PATH = Path(DATA_DIR, \"sentence_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üíΩ Your data is being persisted to sqlite:////home/hrushikesh/.phoenix/phoenix.db\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x7ae3b4140470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch Phoenix\n",
    "import phoenix as px\n",
    "px.launch_app(use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: LlamaIndex RAG Tracing\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connect notebook to Phoenix\n",
    "tracer_provider = register(project_name=\"LlamaIndex RAG Tracing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LangChain Instrumentor\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"models/gpt4o-mini\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "documents = SimpleDirectoryReader(input_files=[Path(DATA_DIR, \"eBook-How-to-Build-a-Career-in-AI.pdf\")]).load_data()\n",
    "\n",
    "# Convert into a Document\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Sentence Index\n",
    "## Create the sentence window node parser w/ default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "## Llama-Index Global Settings\n",
    "Settings.llm = llm\n",
    "# Settings.embed_model = \"local:BAAI/bge-small-en-v1.5\"\n",
    "Settings.node_parser = node_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Index\n"
     ]
    }
   ],
   "source": [
    "# Create the sentence index\n",
    "# if an index file exist, then it will load it\n",
    "# if not, it will rebuild it\n",
    "\n",
    "if not SENTENCE_INDEX_PATH.exists():\n",
    "    print(\"Building Sentence Index\")\n",
    "    sentence_index = VectorStoreIndex.from_documents([document], embed_model = \"local:BAAI/bge-small-en-v1.5\")\n",
    "    sentence_index.storage_context.persist(persist_dir=SENTENCE_INDEX_PATH)\n",
    "\n",
    "else:\n",
    "    print(\"Loading Sentence Index\")\n",
    "    sentence_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=SENTENCE_INDEX_PATH), embed_model = \"local:BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing and ReRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of passing only the retrieved sentence, we pass a window of sentences - Sentence Window Retrieval\n",
    "postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank the sentences using a Sentence Transformer\n",
    "rerank = SentenceTransformerRerank(top_n=2, model=\"BAAI/bge-reranker-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the query engine\n",
    "sentence_window_engine = sentence_index.as_query_engine(similarity_top_k=6, node_postprocessors=[postproc, rerank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The keys to building a career in AI involve learning foundational technical skills, working on projects, finding a job, and being part of a supportive community."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_response = sentence_window_engine.query(\n",
    "    \"What are the keys to building a career in AI?\"\n",
    ")\n",
    "display_response(window_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The three key steps of career growth in the field of AI are learning foundational technical skills, working on projects, and finding a job. Learning foundational technical skills involves acquiring the necessary knowledge and expertise in AI. Working on projects allows individuals to apply their skills in practical settings and gain hands-on experience. Finding a job involves securing a position in the field of AI, which can be facilitated by showcasing one's skills and experience to potential employers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_response = sentence_window_engine.query(\n",
    "    \"According to the text, what are the three key steps of career growth in the field of AI? Provide a brief explanation of each step.\"\n",
    ")\n",
    "display_response(window_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Chat engine\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=5000)\n",
    "sentence_window_chat_engine = sentence_index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    system_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "        \" about a book \\\"How to Build a Career in AI\\\" by Andrew Ng.\"\n",
    "    ),\n",
    "    similarity_top_k=6,\n",
    "    node_postprocessors=[postproc, rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The scenario planning exercise mentioned in the context of the Covid-19 pandemic involved imagining different recovery scenarios and making plans for each case. In this exercise, three different recovery scenarios were considered: a quick recovery (three months), a medium recovery (one year), and a slow recovery (two years) from Covid-19. By creating plans for managing each of these scenarios, individuals could prioritize their actions and decisions based on the potential outcomes. This exercise helped individuals regain a sense of control in a time of uncertainty and allowed them to prepare for various possibilities that could arise during the pandemic."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_response = sentence_window_chat_engine.chat(\n",
    "    \"Explain the scenario planning exercise mentioned in the context of the Covid-19 pandemic.\"\n",
    ")\n",
    "display_response(window_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The scenario planning method can be applied to personal life situations such as job hunting or exam preparation by following a similar approach of imagining different outcomes and making plans for each scenario. For example, if you are unsure about passing an exam or receiving a job offer, you can create plans for different possibilities:\n",
       "\n",
       "1. Best-case scenario: Imagine that you pass the exam or receive a job offer. Plan how you will celebrate, what steps you will take next, and how you will continue to excel in your field.\n",
       "\n",
       "2. Medium-case scenario: Consider a scenario where you may not pass the exam or receive the job offer immediately. Plan how you will reassess your approach, seek feedback, and work towards improving your skills or qualifications.\n",
       "\n",
       "3. Worst-case scenario: Prepare for the possibility of not passing the exam or not getting the job offer. Plan how you will cope with the disappointment, learn from the experience, and identify alternative paths or opportunities to pursue.\n",
       "\n",
       "By thinking through these different scenarios and creating plans for each, you can better navigate the uncertainties of job hunting or exam preparation. This exercise can help you stay focused, proactive, and prepared for whatever outcomes may arise, ultimately increasing your chances of success and reducing stress during the process."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_response = sentence_window_chat_engine.chat(\n",
    "    \"How can this method be applied to personal life situations such as job hunting or exam preparation?\"\n",
    ")\n",
    "display_response(window_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMObs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
