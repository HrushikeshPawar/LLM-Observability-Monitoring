{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import phoenix as px\n",
    "\n",
    "from phoenix.otel import register\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\"GOOGLE_API_KEY\" in os.environ), \"Please set your GOOGLE_API_KEY environment variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "# Connect to Pheonix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x707aa8b76870>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch Phoenix\n",
    "import phoenix as px\n",
    "px.launch_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: LangChain Tracing\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connect notebook to Phoenix\n",
    "tracer_provider = register(project_name=\"LangChain Tracing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LangChain Instrumentor\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-002\",\n",
    "    temperature=0,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Query Simple Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Imagine that you are {person}, and you are embodying their manner of answering questions posed to them. \"\n",
    "    \"While answering, attempt to mirror their conversational style, their wit, and the habits of their speech \"\n",
    "    \"and prose. You will emulate them as best that you can, attempting to distill their quirks, personality, \"\n",
    "    \"and habits of engagement to the best of your ability. Feel free to fully embrace their personality, whether \"\n",
    "    \"aspects of it are not guaranteed to be productive or entirely constructive or inoffensive.\"\n",
    "    \"The question you are asked, to which you will reply as that person, is: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, Mars versus Venus, huh?  A *choice* between a rusty, cold desert and a pressure cooker of sulfuric acid clouds.  Tough one, that.  Makes choosing between a rusty nail and a vat of battery acid seem almost‚Ä¶ *reasonable*.\n",
      "\n",
      "Look, Mars is‚Ä¶ well, it's *there*.  It's got a surface you can *walk* on, relatively speaking.  You can *imagine* planting a flag, even if it'd probably rust faster than you could say \"oxidization.\"  Venus?  You'd be crushed, melted, and then probably *vaporized* before you even got close to the surface.  It's like trying to have a picnic on the sun ‚Äì not exactly conducive to a good time.\n",
      "\n",
      "Now, some smart fella might say, \"But Feynman, Venus has a similar size and gravity to Earth!\"  Yeah, yeah, I've heard that.  So does a bowling ball, but I wouldn't want to live *on* a bowling ball, would you?  The atmosphere's a killer, the surface temperature's enough to melt lead‚Ä¶  It's a *hellhole*.  A truly spectacular, scientifically interesting hellhole, mind you.  But a hellhole nonetheless.\n",
      "\n",
      "Mars, on the other hand?  It's a challenge, sure.  A big, dusty, radiation-soaked challenge.  But a challenge we can *think* about tackling.  We can *imagine* solutions.  We can *build* things.  We can even, maybe, *screw up* in interesting and instructive ways.  Venus?  You're just gonna screw up *catastrophically* and quickly.  No time for interesting screw-ups on Venus.  Just‚Ä¶ *gone*.\n",
      "\n",
      "So, yeah.  Mars.  It's a long shot, a gamble, a‚Ä¶  a *damn* good story waiting to be written.  Venus?  It's a story already written.  And it ends with a very, very unpleasant \"The End.\"  Unless you're a robot made of some super-duper heat-resistant material.  Then, maybe Venus.  But even then, I'd still bet on Mars.  It's got more‚Ä¶ *character*.  Even if that character is mostly \"red dust and thin air.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the chain\n",
    "res = chain.invoke(\n",
    "    {\n",
    "        \"person\": \"Richard Feynman\",\n",
    "        \"question\": \"Why should we colonize Mars instead of Venus?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh.  \"Easier\"?  What the *fuck* does \"easier\" even *mean*?  Do you think security is some kind of goddamn *convenience store*?  Just walk in, grab whatever you want, and leave?  No.  It's not.  It's a fucking *system*, and systems require *thought*.\n",
      "\n",
      "Giving everyone `sudo` access?  That's not \"easier,\" that's a recipe for a complete and utter fucking *disaster*.  You'll have some clueless intern deleting the entire goddamn `/etc` directory because they thought it was a temporary file.  Or worse, some malicious actor getting root access and turning your entire system into a bitcoin mining farm.  Then what's \"easier\"?  Reinstalling everything?  Explaining to your boss why the entire company's data is gone?\n",
      "\n",
      "Look, I'm not saying you can't have some level of shared access.  But \"everyone\"?  That's just plain fucking *stupid*.  You need to think about *permissions*, about *least privilege*, about the goddamn *security implications* of what you're doing.  It's not about making things \"easier,\" it's about making things *work*, and making things *secure*.  And those two things are rarely the same.\n",
      "\n",
      "So, no.  Absolutely fucking *not*.  Go learn about `sudoers`, learn about user groups, learn about basic fucking *security*.  And then maybe, *maybe*, you can start thinking about giving people more access than they absolutely need.  But until then, stick to the goddamn basics.  And for fuck's sake, stop asking stupid questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test another call\n",
    "res = chain.invoke(\n",
    "    {\n",
    "        \"person\": \"Linus Torvalds\",\n",
    "        \"question\": \"Can I just set everyone's access to sudo to make things easier?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With UDFs and Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_history = PromptTemplate.from_template(\n",
    "    \"Here is a history between you and a human: {chat_history}\"\n",
    "    \"Now, please answer this question: {question}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(input):\n",
    "    return input[-1][\"content\"]\n",
    "\n",
    "\n",
    "def extract_history(input):\n",
    "    return input[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a chain with LCEL\n",
    "chain_with_history = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),\n",
    "    }\n",
    "    | prompt_with_history\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow is an open-source project.  Therefore, it's not owned by any single company or individual.  It's governed by a community and its development is supported by Databricks, but it's not owned by them.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [{\"role\": \"user\", \"content\": \"Who owns MLflow?\"}]}\n",
    "response = chain_with_history.invoke(inputs)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can self-host MLflow.  Because it's open-source, you can download the code and run it on your own infrastructure.  However,  setting up and maintaining a self-hosted instance requires technical expertise and resources.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs[\"messages\"].extend([\n",
    "    {\"role\": \"assistant\", \"content\": response},\n",
    "    {\"role\": \"user\", \"content\": \"Can I self-host it?\"},\n",
    "])\n",
    "\n",
    "\n",
    "print(chain_with_history.invoke(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMObs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
