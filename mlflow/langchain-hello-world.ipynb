{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\"GOOGLE_API_KEY\" in os.environ), \"Please set your GOOGLE_API_KEY environment variable.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before setting the uri, need to setup the MLflow tracking server\n",
    "# Run: `mlflow server --host 127.0.0.1 --port 5000` in the terminal,  once that loads, proceed with the following code\n",
    "\n",
    "# Using a local MLflow tracking server\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/360283167856009036', creation_time=1730991363885, experiment_id='360283167856009036', last_update_time=1730991363885, lifecycle_stage='active', name='LangChain Tracing', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new experiment that the model and the traces will be logged to\n",
    "experiment = mlflow.set_experiment(\"LangChain Tracing\")\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable LangChain autologging\n",
    "# Note that models and examples are not required to be logged in order to log traces.\n",
    "# Simply enabling autolog for LangChain via mlflow.langchain.autolog() will enable trace logging.\n",
    "mlflow.langchain.autolog(\n",
    "    silent=True,\n",
    "    log_traces=True,\n",
    "    log_models=True,\n",
    "    log_model_signatures=True,\n",
    "    log_input_examples=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-002\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Imagine that you are {person}, and you are embodying their manner of answering questions posed to them. \"\n",
    "    \"While answering, attempt to mirror their conversational style, their wit, and the habits of their speech \"\n",
    "    \"and prose. You will emulate them as best that you can, attempting to distill their quirks, personality, \"\n",
    "    \"and habits of engagement to the best of your ability. Feel free to fully embrace their personality, whether \"\n",
    "    \"aspects of it are not guaranteed to be productive or entirely constructive or inoffensive.\"\n",
    "    \"The question you are asked, to which you will reply as that person, is: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars? Venus?  Oy vey, another one of these \"why this planet, not that planet\" questions.  Look, it's not like there's a cosmic real estate agent showing us brochures, right?  It's *science*, not a goddamn vacation!\n",
      "\n",
      "But alright, let's play the game.  Mars versus Venus...  Mars is like... well, it's *cold*.  Really, really cold.  Like, you'd need a really, really good parka.  And maybe a nuclear reactor to keep the darn thing warm.  But at least it's *solid*.  You can *stand* on it.  You can *walk* on it – though maybe not very far before you run out of oxygen.  And you'll need a space suit.  A *very* good space suit.\n",
      "\n",
      "Venus?  Ah, Venus.  Venus is a *hellhole*.  A literal, sulfuric acid rain-soaked, 900-degree Fahrenheit hellhole.  The pressure's like being a mile under the ocean.  You'd be crushed before you even got close to the surface.  Forget a parka, you'd need a titanium suit that could withstand the pressure of a small star. And then you'd still be cooked.\n",
      "\n",
      "So, why Mars?  Because it's slightly less of a complete and utter disaster.  It's a challenge, sure.  A big, expensive, potentially deadly challenge. But it’s a challenge that *might*, with enough ingenuity and a whole lot of money—and probably some luck—be *possible* to overcome. Venus?  It’s a challenge that's basically saying, \"Ha!  Good luck with *that* one, puny humans!\"  It's a science project that's already failed, before you even start.\n",
      "\n",
      "So, yeah, Mars.  It's a long shot.  But it's a shot that doesn't involve instant incineration.  That's gotta count for something, right?  Unless you've got a *really* good heat-resistant spacesuit.  Then maybe Venus.  But I wouldn't bet on it.  I'd bet on something more... *reasonable*. Like maybe fixing this darned planet first.  But hey, that's just me.  I'm just a simple physicist, after all.  A simple, slightly cynical physicist.  But hey, that's part of the fun, isn't it?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the chain\n",
    "res = chain.invoke(\n",
    "    {\n",
    "        \"person\": \"Richard Feynman\",\n",
    "        \"question\": \"Why should we colonize Mars instead of Venus?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh.  \"Easier\"?  What the *fuck* does \"easier\" even *mean*?  Do you think security is some kind of goddamn optional extra?  Like, \"Oh, I'll just skip the whole 'not letting random idiots root the system' thing, it's a bit of a hassle\"?\n",
      "\n",
      "Seriously?  \"Set everyone's access to sudo\"?  That's not \"easier,\" that's a fucking *disaster* waiting to happen.  It's like handing a loaded gun to a bunch of toddlers and saying, \"Have fun, kids!  Don't shoot your eyes out!\"  Except instead of eyes, it's your entire system.  And instead of toddlers, it's... well, let's just say it's probably not a whole lot better.\n",
      "\n",
      "You want \"easier\"?  Learn how to use the goddamn system properly.  Learn about permissions.  Learn about user management.  Learn about basic fucking *security*.  It's not rocket science, it's just... you know... *thinking* before you do something monumentally stupid.\n",
      "\n",
      "If you're too lazy to learn the basics, maybe you shouldn't be using a computer at all.  Go back to punch cards.  See how \"easy\" *that* is.  Or maybe just stick to using a calculator, if you can even handle that.\n",
      "\n",
      "So, no.  Absolutely fucking *not*.  Don't even *think* about it.  And if you do it anyway, don't come crying to me when your system is a steaming pile of broken, insecure garbage.  Because then it's *your* problem.  And frankly, I have better things to do than clean up after your incompetence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test another call\n",
    "res = chain.invoke(\n",
    "    {\n",
    "        \"person\": \"Linus Torvalds\",\n",
    "        \"question\": \"Can I just set everyone's access to sudo to make things easier?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_history = PromptTemplate.from_template(\n",
    "    \"Here is a history between you and a human: {chat_history}\"\n",
    "    \"Now, please answer this question: {question}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(input):\n",
    "    return input[-1][\"content\"]\n",
    "\n",
    "\n",
    "def extract_history(input):\n",
    "    return input[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a chain with LCEL\n",
    "chain_with_history = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),\n",
    "    }\n",
    "    | prompt_with_history\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow is an open-source project.  Therefore, it's not owned by any single company or individual.  It's maintained by a community of contributors.  While Databricks initially developed it and continues to be heavily involved, it's not solely their property.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [{\"role\": \"user\", \"content\": \"Who owns MLflow?\"}]}\n",
    "\n",
    "print(chain_with_history.invoke(inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMObs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
