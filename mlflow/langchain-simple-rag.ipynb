{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with MLflow and LangChain\n",
    "\n",
    "Tutorial can be found at: [MLflow LangChain Retriever](https://mlflow.org/docs/latest/llms/langchain/notebooks/langchain-retriever.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\"GOOGLE_API_KEY\" in os.environ), \"Please set your GOOGLE_API_KEY environment variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Models\n",
    "google_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-002\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "google_llm_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model='models/text-embedding-004',\n",
    "    task_type=\"retrieval_document\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup MLflow\n",
    "## Tracking URI\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "## Create a new experiment\n",
    "experiment = mlflow.set_experiment(\"LangChain RAG Tracing\")\n",
    "\n",
    "## Enable LangChain autologging\n",
    "mlflow.langchain.autolog(\n",
    "    silent=True,\n",
    "    log_traces=True,\n",
    "    log_models=True,\n",
    "    log_model_signatures=True,\n",
    "    log_input_examples=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Federal Documents\n",
    "def fetch_federal_document(url, div_class):\n",
    "    \"\"\"\n",
    "    Scrapes the transcript of the Act Establishing Yellowstone National Park from the given URL.\n",
    "\n",
    "    Args:\n",
    "    url (str): URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "    str: The transcript text of the Act.\n",
    "    \"\"\"\n",
    "    # Sending a request to the URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Parsing the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Finding the transcript section by its HTML structure\n",
    "        transcript_section = soup.find(\"div\", class_=div_class)\n",
    "        if transcript_section:\n",
    "            transcript_text = transcript_section.get_text(separator=\"\\n\", strip=True)\n",
    "            return transcript_text\n",
    "        else:\n",
    "            return \"Transcript section not found.\"\n",
    "    else:\n",
    "        return f\"Failed to retrieve the webpage. Status code: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Fetching and Saving\n",
    "def fetch_and_save_documents(url_list, doc_path):\n",
    "    \"\"\"\n",
    "    Fetches documents from given URLs and saves them to a specified file path.\n",
    "\n",
    "    Args:\n",
    "        url_list (list): List of URLs to fetch documents from.\n",
    "        doc_path (str): Path to the file where documents will be saved.\n",
    "    \"\"\"\n",
    "    for url in url_list:\n",
    "        document = fetch_federal_document(url, \"col-sm-9\")\n",
    "        with open(doc_path, \"a\") as file:\n",
    "            file.write(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS Database Creation\n",
    "def create_faiss_database(document_path, database_save_directory, chunk_size=500, chunk_overlap=10):\n",
    "    \"\"\"\n",
    "    Creates and saves a FAISS database using documents from the specified file.\n",
    "\n",
    "    Args:\n",
    "        document_path (str): Path to the file containing documents.\n",
    "        database_save_directory (str): Directory where the FAISS database will be saved.\n",
    "        chunk_size (int, optional): Size of each document chunk. Default is 500.\n",
    "        chunk_overlap (int, optional): Overlap between consecutive chunks. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        FAISS database instance.\n",
    "    \"\"\"\n",
    "    # Load documents from the specified file\n",
    "    document_loader = TextLoader(document_path)\n",
    "    raw_documents = document_loader.load()\n",
    "\n",
    "    # Split documents into smaller chunks with specified size and overlap\n",
    "    document_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    document_chunks = document_splitter.split_documents(raw_documents)\n",
    "\n",
    "    # Generate embeddings for each document chunk\n",
    "    embedding_generator = google_llm_embeddings\n",
    "    faiss_database = FAISS.from_documents(document_chunks, embedding_generator)\n",
    "\n",
    "    # Save the FAISS database to the specified directory\n",
    "    faiss_database.save_local(database_save_directory)\n",
    "\n",
    "    return faiss_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory to store documents\n",
    "directory = Path('..', 'data')\n",
    "\n",
    "# Document Paths and FAISS Index Directory\n",
    "doc_path = os.path.join(directory, \"docs.txt\")\n",
    "persist_dir = os.path.join(directory, \"faiss_index_langchain_simple_rag\")\n",
    "\n",
    "# URLs of the Federal documents to scrape\n",
    "url_listings = [\n",
    "    \"https://www.archives.gov/milestone-documents/act-establishing-yellowstone-national-park#transcript\",\n",
    "    \"https://www.archives.gov/milestone-documents/sherman-anti-trust-act#transcript\",\n",
    "]\n",
    "\n",
    "# Fetch and save documents from the URLs\n",
    "if not os.path.exists(doc_path):\n",
    "    print(\"Fetching and saving documents...\")\n",
    "    fetch_and_save_documents(url_listings, doc_path)\n",
    "\n",
    "# Create a FAISS database from the saved documents\n",
    "faiss_index = create_faiss_database(doc_path, persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Demo Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup RetrievalQA\n",
    "retrievalQA = RetrievalQA.from_llm(llm=google_llm, retriever=faiss_index.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/11 13:59:14 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpl4ercc9w/model, flavor: langchain). Fall back to return ['langchain==0.3.7', 'pydantic==2.9.2', 'cloudpickle==3.1.0']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/11/11 13:59:14 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run smiling-fish-215 at: http://localhost:5000/#/experiments/584542048209027571/runs/6507f4194a0b454a9c420fbfc29f6568.\n",
      "2024/11/11 13:59:14 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://localhost:5000/#/experiments/584542048209027571.\n"
     ]
    }
   ],
   "source": [
    "def load_retriever(persist_directory):\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(\n",
    "        model='models/text-embedding-004',\n",
    "        task_type=\"retrieval_document\",\n",
    "    )\n",
    "    vectorstore = FAISS.load_local(\n",
    "        persist_directory,\n",
    "        embeddings,\n",
    "        # you may need to add the line below\n",
    "        # for langchain_community >= 0.0.27\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    logged_model = mlflow.langchain.log_model(\n",
    "        retrievalQA,\n",
    "        artifact_path=\"retrieval_qa\",\n",
    "        loader_fn=load_retriever,\n",
    "        persist_dir=persist_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! This code is broken and doesn't work\n",
    "# loaded_model = mlflow.pyfunc.load_model(logged_model.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_formatted_response(response_list, max_line_length=80):\n",
    "    \"\"\"\n",
    "    Formats and prints responses with a maximum line length for better readability.\n",
    "\n",
    "    Args:\n",
    "    response_list (list): A list of strings representing responses.\n",
    "    max_line_length (int): Maximum number of characters in a line. Defaults to 80.\n",
    "    \"\"\"\n",
    "    for response in response_list:\n",
    "        words = response.split()\n",
    "        line = \"\"\n",
    "        for word in words:\n",
    "            if len(line) + len(word) + 1 <= max_line_length:\n",
    "                line += word + \" \"\n",
    "            else:\n",
    "                print(line)\n",
    "                line = word + \" \"\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document states that all persons who locate, settle upon, or occupy a \n",
      "public park or pleasuring-ground, except as otherwise provided, shall be \n",
      "considered trespassers and removed. Additionally, it mentions that a park \n",
      "warden will remove trespassers after the passage of the act. \n"
     ]
    }
   ],
   "source": [
    "answer1 = retrievalQA.invoke({\"query\": \"What does the document say about trespassers?\"})\n",
    "\n",
    "print_formatted_response([answer1['result']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but this text does not contain information about bridle paths or \n",
      "whether they exist in Yellowstone National Park. I cannot answer your question. \n"
     ]
    }
   ],
   "source": [
    "answer2 = retrievalQA.invoke({\"query\": \"What is a bridle-path and can I use one at Yellowstone?\"})\n",
    "\n",
    "print_formatted_response([answer2['result']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    google_llm, faiss_index.as_retriever(), contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(google_llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code doesn't work with this rag_chain object, need to dig into the documentation. One of my reason to not use these wrappers in production!!!\n",
    "# # def load_retriever(persist_directory):\n",
    "# #     embeddings = GoogleGenerativeAIEmbeddings(\n",
    "# #         model='models/text-embedding-004',\n",
    "# #         task_type=\"retrieval_document\",\n",
    "# #     )\n",
    "# #     vectorstore = FAISS.load_local(\n",
    "# #         persist_directory,\n",
    "# #         embeddings,\n",
    "# #         # you may need to add the line below\n",
    "# #         # for langchain_community >= 0.0.27\n",
    "# #         allow_dangerous_deserialization=True,\n",
    "# #     )\n",
    "# #     return vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# # with mlflow.start_run() as run:\n",
    "# #     logged_model = mlflow.langchain.log_model(\n",
    "# #         rag_chain,\n",
    "# #         artifact_path=\"rag_chain_chat\",\n",
    "# #         loader_fn=load_retriever,\n",
    "# #         persist_dir=persist_dir,\n",
    "# #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document describes a tract of land near the headwaters of the Yellowstone \n",
      "River in Montana and Wyoming, reserving it as a public park. This land is \n",
      "described by its boundaries, starting at the junction of Gardiner's River and \n",
      "the Yellowstone River. The act withdraws the land from settlement, occupancy, \n",
      "or sale. \n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"What is the document say about the Yellowstone?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print_formatted_response([ai_msg_1['answer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, the document explicitly states that the land is withdrawn from sale under \n",
      "the laws of the United States. \n"
     ]
    }
   ],
   "source": [
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"Can I buy it from the Fedral Government?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print_formatted_response([ai_msg_2[\"answer\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMObs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
